<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yet another blog</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://mkhurram.com/"/>
  <updated>2016-09-03T13:34:30.638Z</updated>
  <id>http://mkhurram.com/</id>
  
  <author>
    <name>Khurram Amin</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Convolutional Layer in CNN</title>
    <link href="http://mkhurram.com/2016/09/03/an-introduction-to-convolutional-layer/"/>
    <id>http://mkhurram.com/2016/09/03/an-introduction-to-convolutional-layer/</id>
    <published>2016-09-03T04:29:25.000Z</published>
    <updated>2016-09-03T13:34:30.638Z</updated>
    
    <content type="html"><![CDATA[<!-- toc -->

<h1 id="convolutional-layer">Convolutional Layer</h1>
<p>A very informative tutorial on Convolutional Neural Networks by <a href="http://cs.stanford.edu/people/karpathy/" target="_blank" rel="external">Andrej Karpath</a> can be found here <a href="https://www.youtube.com/watch?v=V8JDMkARdfU&amp;index=7&amp;list=PLlJy-eBtNFt6EuMxFYRiNRS07MCWN5UIA]" target="_blank" rel="external">Video</a>, <a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="external">Lecture Notes</a>, <a href="http://cs231n.stanford.edu/slides/winter1516_lecture7.pdf" target="_blank" rel="external">Slides</a>.</p>
<p><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network" target="_blank" rel="external">Wikipedia</a> also has a very detailed article explaining the working of CNN.</p>
<h2 id="min-summary">2-min Summary</h2>
<p>The basic purpose of introducing Convolutional Layer (<strong>CL</strong>) in Neural Network (<strong>NN</strong>) architecture is: 1. To incorporate Translational Invariance 2. To exploit (spatial/temporal) local correlations</p>
<p>Added advantages of CL are: 1. Reduction in number of learnable parameters (<strong>LP</strong>) 2. Avoids duplicate nodes (Neuron(s) with similar parameters) 3. A better understanding of what network is learning</p>
<p>A convolutional layer learns an <span class="math">\(H*W\)</span> filter which is convolved with the whole input from previous layer with stride (shift/step) <strong>S</strong> and resulting output is accumulated into a 2D matrix (plane). This output plane is called a <strong>feature map</strong>. A CL can be configured to produce <strong>N</strong> feature maps. Usually the learnable mask is kept square i.e. <span class="math">\(H*H\)</span>.</p>
<p>[A visualization of a single dimensional convolutional operation][figure0] [figure0]: http://cs231n.github.io/assets/cnn/stride.jpeg Figure 0: 1D convolutional operation. The gray boxes are input, green box on right is the convolutional kernel. On left, kernel convolves with the input with stride 1 and produces the output in yellow. On right, same kernel convolves with the same input with stride 2 to produce out in green. (image is taken from Stanford’s CS231n lecture notes)</p>
<p>Parameters of CL <span class="math">\[
\begin{eqnarray} 
H &amp;= \text( size of (square) filter, How big the learned filter/mask should be? ) \\
S &amp;= \text(stride (step/shift), What should be the step size during convolution operation?) \\
N &amp;= \text(number of feature maps, How many filters/masks should be learned?) \\
P &amp;= \text(Padding size, How many rows and columns should be inserted on borders of input volume/image/plane?) \\
\end{eqnarray}  
\]</span></p>
<h2 id="indepth-review">Indepth Review</h2>
<p>[Fully Connected Network][figure1] [figure1]: http://cs231n.github.io/assets/nn1/neural_net2.jpeg Figure 1: A fully connected neural network with 2 hidden layers. (image is taken from Stanford’s CS231n lecture notes)</p>
<p>A NN can be designed in fully connected fashion i.e. each neuron in each layer takes every output of previous layer as input. While such an architecture will be simple to design but it will have its drawbacks.</p>
<p>First, the number of LP in such a network will explode exponentially. For a recognition task if an <span class="math">\(H*W\)</span> image is used as input and network is designed in FC fashion then the number of LP in first hidden layer will be <span class="math">\(H*W*N,\)</span> whereas N is the number of nodes in first hidden layer. For a typical patch of size 100x100 and a hidden layer with 100 nodes, the LP in first layer will be 0.1 million. Although such huge number of parameters will increase the learning capacity of our learning machine/system but to learn such a large number of parameters will require even larger dataset. Availability of larger (gigantic) dataset cannot be ensured for every learning problem. This scenario becomes more drastic in supervised learning tasks where annotations are also required for training.</p>
<p>Second, the incorporation of prior knowledge about problem in learning machine/system can improve system accuracy. This fact can be of greater importance if training data is scarcer. In FC network either no-prior knowledge is incorporated or if one wants to incorporate prior-knowledge he has to do this in pre or post processing steps. This will either result in sub-par learning (in case of no prior-knowledge incorporation) or as a cumbersome and potentially more time consuming system. Moreover another draw back of pre/post processing is sub-optimality of such a process if designed by hand. In case of images one such prior-knowledge is correlation between spatially nearby pixels. ####Insert images of objects where local information is enough to take some decision about the scene/object/image &amp; do some discussion on them.</p>
<p>Third, geometric transformations applied on input image (object) should not have any effect on the output of network. One such geometric transformation is 2D translation i.e. an object can appear, in same pose, anywhere in the input image. In FC network architecture, this phenomenon is captured by learning not only the appearance of object but also the all possible locations in the input field where the object can exist. As a result many nodes will have same/similar parameters and will respond same object appearing in different spatial locations. This will not require larger training set i.e. all combinations of transformed images (objects) must be provided to build/learn invariance to the geometric transformations in learning system.</p>
<p>Four, complex objects (structures) are made of simpler objects (shapes). A house can be made by arranging a set of 11 lines in a <em>particular</em> order. So it will be useful if our learning system has the ability to extract these elementary objects and a way to combine them to learn the ultimate shape of object. In FC network it cannot be ensured that the network will always learn this hierarchical structure. ######Insert a figure of a line drawing of a house here</p>
<h3 id="what-is-a-convolutional-layer">What is a Convolutional Layer</h3>
<p>[A Convolutional Neural Network][figure2] [figure2]: http://cs231n.github.io/assets/cnn/cnn.jpeg Figure 2: A convolutional neural network with 3 convolutional layers. (image is taken from Stanford’s CS231n lecture notes)</p>
<p>In CL each neuron is connected to a spatially local, <span class="math">\(Hc*Wc\)</span>, patch of previous layer. It takes dot product of a <span class="math">\(Hc*Wc\)</span> patch and a similar sized weight matrix, adds some scalar bias and outputs a scalar value. This spatially small neighborhood of size <span class="math">\(Hc*Wc\)</span> in previous layer is called <strong>receptive field</strong> of the neuron. To get output from other locations of the input image, this node is repeated (with same weights/LP) and is fed with translated patches/windows. These repeated neurons are arranged in 2D plane. This plane is called <strong>feature map</strong>. In a feature map every node have same parameters as other nodes.</p>
<p>[A Convolutional Layer][figure3] [figure3]: http://cs231n.github.io/assets/cnn/depthcol.jpeg Figure 3: A convolutional layer operating on an input of <span class="math">\(32*32*3\)</span>, shown in lighter red, the convolutional mask, shown in darker red and the output feature volume shown in blue. (image is taken from Stanford’s CS231n lecture notes)</p>
<p>[Maths of a Convolutional Layer][figure4] [figure4]: http://cs231n.github.io/assets/nn1/neuron_model.jpeg Figure 4: Each node in a convolutional layer takes a set of feature vector <span class="math">\(x\)</span> as input, computes its dot product with a similar size weights vector <span class="math">\(w\)</span>, adds a scalar bias b and outputs a scalar value <span class="math">\(f(x,w, b)\)</span>. (image is taken from Stanford’s CS231n lecture notes)</p>
<ol style="list-style-type: decimal">
<li>Number of parameters in a single node = <span class="math">\(H_c * W_c * N_c + 1\)</span>, whereas Nc is the number of channels in input volume and the kernel</li>
<li>Number of nodes in a feature map = $  * $</li>
<li>Number of parameters and connections in a feature map (without padding the input) = <span class="math">\(\frac{H - H_c + 1}{S_h} * \frac{W - W_c + 1}{S_w} * H_c * W_c * N_c + 1 \)</span></li>
<li>Number of free/LP in a feature map = <span class="math">\(H_c * W_c * N_c + 1\)</span></li>
<li>In practice some kind of padding is done on the borders of input volume to avoid reducing the width and height of output volume during convolutional operation. In such a case the number of nodes and total parameters and connections change accordingly while the number of LP remain same.</li>
</ol>
<p>Another key point to note over here is that a single convolutional filter operates on the entire depth of the input volume and thus have the same depth as input volume. While the depth of output volume can be controlled by number of features parameter.</p>
<iframe src="//cs231n.github.io/assets/conv-demo/index.html" width="100%" height="680px;" style="border:none;"></iframe>
<p>Figure 6: The above figure shows a 3 channel input of size 5x5. Each channel is stacked in a column separately on left. The 5x5 input is zero-padded with p = 1. The stride is 2. The first red column from left shows the first 3x3x3 kernel whereas each channel is stacked over one another. Similarly the second red column from red shows the second 3x3x3 convolutional filter. The green box on right shows the output of convolutional operation. (This visualization is taken from Stanford’s CS231n course notes).</p>
<p>[Some sample learned feature kernels][figure7] [figure7]: http://cs231n.github.io/assets/cnn/weights.jpeg Figure 7: Each block in above figure shows a feature kernel learned by the first layer of an example convolutional neural network There are 96 kernels. (image is taken from Stanford’s CS231n lecture notes)</p>
<h3 id="how-to-convolutional-layer-removes-some-of-above-mentioned-drawbacks">How to convolutional layer removes some of above mentioned drawbacks?</h3>
<h3 id="code-examples">Code examples</h3>
<h3 id="references">References</h3>
]]></content>
    
    <summary type="html">
    
      &lt;!-- toc --&gt;

&lt;h1 id=&quot;convolutional-layer&quot;&gt;Convolutional Layer&lt;/h1&gt;
&lt;p&gt;A very informative tutorial on Convolutional Neural Networks by &lt;a hr
    
    </summary>
    
    
      <category term="Computer Vision" scheme="http://mkhurram.com/tags/Computer-Vision/"/>
    
      <category term="Machine Learning" scheme="http://mkhurram.com/tags/Machine-Learning/"/>
    
      <category term="Deep Learning" scheme="http://mkhurram.com/tags/Deep-Learning/"/>
    
      <category term="Tutorial" scheme="http://mkhurram.com/tags/Tutorial/"/>
    
  </entry>
  
  <entry>
    <title>How to count number of pilgrims in 27th Ramazan image</title>
    <link href="http://mkhurram.com/2016/08/10/How-to-count-number-of-pilgrims-in-27th-Ramazan-image/"/>
    <id>http://mkhurram.com/2016/08/10/How-to-count-number-of-pilgrims-in-27th-Ramazan-image/</id>
    <published>2016-08-10T19:00:00.000Z</published>
    <updated>2016-08-13T04:17:15.074Z</updated>
    
    <content type="html"><![CDATA[<!-- endcontent --><div class="figure figure--fullWidth"><img class="figure-img" src="https://github.com/khurram-amin/public_on_web2/blob/master/H2cnopi27ri/3.jpg?raw=true" alt=""><span class="caption"></span></div><!-- content -->
<p>Try to count number of persons in this image. Got any guess? Post in comments. This image was taken on 03 July 2016 (On the eve of 26th Ramadan after Taraweh) at Masjid-Al-Haram, Mecca, KSA.</p>
<!-- toc -->

<p>The problem of counting number of objects in scenes, similar to the above one, is far from being solved. Even the state-of-art method can detect upto 2k-5k objects in a single image. So what about the scenes where number of objects (persons in this scene) are of order 1e4–1e5?</p>
<h2 id="how-about-detecting-counting-heads">How about Detecting &amp; Counting Heads?</h2>
<ol start="0" style="list-style-type: decimal">
<li>Generate training data</li>
<li>Train a classifier to detect human head</li>
<li>Generate prospective-incorporated-multi-scalar-patches from this image</li>
<li>Pass through the trained classifier</li>
<li>Visualize</li>
<li>Analyse results</li>
<li>(Optional) Generate a density map</li>
</ol>
<h3 id="training-data">Training Data</h3>
<p>As training data we used some of the images in UCF dataset and one manually annotated earlier image of Masjid-al-Haram. The sample images are shown below: <div class="figure fig-100" style="width:;"><img class="fig-img" src="https://github.com/khurram-amin/public_on_web2/blob/master/H2cnopi27ri/pjimage.jpg?raw=true" alt=""></div> <div class="figure fig-100" style="width:;"><img class="fig-img" src="https://github.com/khurram-amin/public_on_web2/blob/master/H2cnopi27ri/manualAnnot.png?raw=true" alt=""></div></p>
<figure class="codeblock codeblock--tabbed"><figcaption><ul class="tabs"><li class="tab active">main</li><li class="tab">createPatches</li></ul></figcaption><div class="tabs-content"><figure class="highlight plain" style="display: block;"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">im = load image</span><br><span class="line">annotaitons = load annotations file</span><br><span class="line">// Size of patch in pixels. A square window of size patch_size*patch_Size will be generated with center annoations(i,:)</span><br><span class="line">patch_sizes = 10:10:100;</span><br><span class="line"></span><br><span class="line">for i = 1:numel(patch_sizes)</span><br><span class="line">	p = extractPatches (im, annotations, patch_sizes(i));</span><br><span class="line">	save patches in p</span><br></pre></td></tr></tbody></table></figure><figure class="highlight plain" style="display: none;"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">function p = createPatches (image, annotations, patch_size )</span><br><span class="line">{</span><br><span class="line">	p = {[]};</span><br><span class="line">	for i = 1 : size ( annotations, 1 )</span><br><span class="line">		p(end+1) = { extractPatch(image, annotations(i, :), patch_size) };</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure></div></figure>

<p>Total number of patches generated in this process are <strong>310987 (292770 Heads, 18217 Non-heads)</strong> At the end the patches were stored as an LMDB file with 75% training set and 25% validation set. File can be downloaded from <a href="">here</a>.</p>
<h3 id="train-classifier">Train Classifier</h3>
<p>To detect human heads, varying in size from 5x5 pixels to 50x50 pixels, we choose LeNet architecture. LeNet is a simple network which has shown great effectiveness in learning hand written digits from 0-to-9 under background noise. Our assumption here is that human head is a very simple in structure so LeNet should work fine in extracting and seperating the underlying features of a human head from an image.</p>
]]></content>
    
    <summary type="html">
    
      &lt;!-- endcontent --&gt;&lt;div class=&quot;figure figure--fullWidth&quot;&gt;&lt;img class=&quot;figure-img&quot; src=&quot;https://github.com/khurram-amin/public_on_web2/blob/ma
    
    </summary>
    
    
      <category term="Computer Vision" scheme="http://mkhurram.com/tags/Computer-Vision/"/>
    
      <category term="Crowd" scheme="http://mkhurram.com/tags/Crowd/"/>
    
      <category term="Machine Learning" scheme="http://mkhurram.com/tags/Machine-Learning/"/>
    
      <category term="Experiments" scheme="http://mkhurram.com/tags/Experiments/"/>
    
  </entry>
  
  <entry>
    <title>Learning to Count Objects in Images by Lempitsky &amp; Zisserman</title>
    <link href="http://mkhurram.com/2016/08/02/Learning-to-Count-Objects-in-Images/"/>
    <id>http://mkhurram.com/2016/08/02/Learning-to-Count-Objects-in-Images/</id>
    <published>2016-08-02T19:00:00.000Z</published>
    <updated>2016-08-03T15:15:43.158Z</updated>
    
    <content type="html"><![CDATA[<p>Download PDF from <a href="http://papers.nips.cc/paper/4043-learning-to-count-objects-in-images.pdf" target="_blank" rel="external">here</a>. Paper Citations = <a href="https://scholar.google.com.pk/scholar?q=learning+to+count+objects+in+images" target="_blank" rel="external">148</a>. Published in <strong>Advances in Neural Information Processing Systems</strong> in <strong>2010</strong>.</p>
<!--toc-->

<h1 id="minute-summary">1-minute Summary</h1>
<p>Number of objects in a scene can be determined using standard regression techniques. Some features are extracted from an image and then a linear transformation that transforms the features into count is learned by minimizing a cost function. In this paper, authors propose a novel distance measure which coupled with sparse annotations of objects in images can be used to solve this learning problem more efficently.</p>
<h1 id="summary">Summary</h1>
<h2 id="assumptions">Assumptions</h2>
<span class="highlight-text danger">Suppose there exists some feature set $x$ such that its linear transformation maps to the ground-truth (object/crowd) density function i.e. $ F_i^0(p) = w^T x_p^i $ whereas $x_p^i$ is feature vector at pixel $p$ in image $i$. </span>

<h2 id="methodology">Methodology</h2>
<h3 id="input">Input</h3>
<ol style="list-style-type: decimal">
<li><p>A set of images <span class="math">\(I_1, I_2, ..., I_N\)</span></p></li>
<li><p>Computed features <span class="math">\(x_p^i\)</span> for each pixel <span class="math">\(p\)</span> in each image <span class="math">\(i\)</span> in the set</p></li>
<li><p>Annotations: a set of 2D points <span class="math">\(\textbf{P}_i = \{ P_1, P_2, ..., P_{C(i)} \} \)</span> representing the presence of object. <span class="math">\(C(i)\)</span> is the total count of objects in image <span class="math">\(i\)</span></p></li>
</ol>
<h3 id="how-to-automatically-count-objects">How to automatically count objects?</h3>
<p>The linear transformation, that can convert <span class="math">\(x_p^i\)</span> to <span class="math">\(F_i^0(p)\)</span> can be learned by solving the following equation <span class="math">\[
w = \underset{w}{\arg\min} \bigg( w^T w + \lambda \sum_{i=1}^{N} dist\big( F_i^0(.), F_i(. \vert w)\big ) \bigg)
\]</span> Once the transformation parameters <span class="math">\(w\)</span> are known, count for any new image can be determined using <span class="math">\[
\text{number of objects in image I} = \sum_{p \in I} w^T x_p
\]</span></p>
<h3 id="how-to-measure-dist.-.">How to measure <span class="math">\(dist(. , .)\)</span></h3>
<p>Authors have defined a novel distance measure known as <em>MESA: Maximum Excess over SubArrays</em>. Given an image <span class="math">\(I\)</span>, the MESA distance <span class="math">\(D_{MESA}\)</span> between two functions <span class="math">\(F_1(p)\)</span> and <span class="math">\(F_2(p)\)</span> is defined as the largest absolute difference between sums of <span class="math">\(F_1(p)\)</span> and <span class="math">\(F_2(p)\)</span> over all box subarrays in <span class="math">\(I\)</span>: <span class="math">\[
D_{MESA}( F_1, F_2 ) = \underset{B \in \textbf{B} }{max} \bigg \vert \sum_{p \in B} F_1(p) - \sum_{p \in B} F_2(p) \bigg \vert
\]</span> To compute <span class="math">\(D_{MESA}\)</span> they take multiple windows (rectangles) over the image known as box hypothesis <strong>B</strong> then <span class="math">\(D_{MESA}\)</span> is simply the sum of absolute difference between two windows (rectangles) that differ most.</p>
<h3 id="how-to-establish-ground-truth-density">How to establish ground-truth density?</h3>
<p>Since objects (crowd) are discrete in nature i.e. there does exists a well defined boundary, in physical world, between two persons then it is easier to individually mark (with a dot) each object separately. But the process of converting this sparse marking into a continuous heat-map is not trivial. Authors have defined the object-density-map as <span class="math">\[
\forall p \in I_i,  F_i^0(p) = \sum_{P \in \textbf{P}_i} Normal(p; P, \sigma^2 1_{2 \times 2})
\]</span> Here, <span class="math">\(p\)</span> denotes a pixel, <span class="math">\(Normal(p; P, \sigma^2 1_{2 \times 2})\)</span> denotes a normalized Gaussian kernel evaluated at pixel <span class="math">\(p\)</span> with mean at the user placed dot <span class="math">\(P\)</span> and an isotropic covariance matrix with <span class="math">\(\sigma\)</span> being a small value.</p>
<!--Given a set of images $I_1, I_2, ..., I_N$ and a ground-truth density map $F_i^0(p)$ for each image $i$ such that $\sum_{p \in I_i} F_i^0(p)$ equals the total number of objects present in the image then a set of parameters $w$ can be learned such that the mapping function $F_i(p \vert w)$ results in minimum error over the whole dataset. -->

<h2 id="results">Results</h2>
<div class="figure fig-100" style="width:;"><img class="fig-img" src="https://github.com/khurram-amin/public_on_web2/blob/master/l2coiibvlaaz/results.png?raw=true" alt=""></div>


<div class="figure fig-100" style="width:;"><img class="fig-img" src="https://github.com/khurram-amin/public_on_web2/blob/master/l2coiibvlaaz/qualitative_result.png?raw=true" alt=""></div>



<h1 id="key-insights">Key Insights</h1>
<ol style="list-style-type: decimal">
<li>The usage of sparse annotations to solve counting-as-regression-problem can help to increase dataset size by using different box hypothesis over the same image.</li>
</ol>
<h1 id="criticism">Criticism</h1>
<ol style="list-style-type: decimal">
<li><p>Mean Absolute Error (mAE) for object counting is not a good error measure since it entirely hides the complexity of scene. For example a mAE of 1 object/image corresponds to ~0.1% error if there were 1000 objects present in scene. But if there were only 0, 1 or 2 objects then the same measure corresponds to ~50% error.</p></li>
<li><p>The assumption that there exists a linearly-separable-feature-space is very <span class="highlight-text orange">strict </span> and is not valid for complex scene structures.</p></li>
</ol>
<!-- # Applications -->



]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Download PDF from &lt;a href=&quot;http://papers.nips.cc/paper/4043-learning-to-count-objects-in-images.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;here&lt;
    
    </summary>
    
    
      <category term="Computer Vision" scheme="http://mkhurram.com/tags/Computer-Vision/"/>
    
      <category term="Crowd" scheme="http://mkhurram.com/tags/Crowd/"/>
    
      <category term="Machine Learning" scheme="http://mkhurram.com/tags/Machine-Learning/"/>
    
      <category term="Reviews" scheme="http://mkhurram.com/tags/Reviews/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://mkhurram.com/2016/08/02/hello-world/"/>
    <id>http://mkhurram.com/2016/08/02/hello-world/</id>
    <published>2016-08-02T17:11:48.838Z</published>
    <updated>2016-08-02T17:11:48.838Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
    
    </summary>
    
    
  </entry>
  
</feed>
